{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4781724,"sourceType":"datasetVersion","datasetId":2767755},{"sourceId":6555828,"sourceType":"datasetVersion","datasetId":3788284},{"sourceId":10042444,"sourceType":"datasetVersion","datasetId":6130416}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics\n!pip install -U ipywidgets\nfrom ultralytics import YOLO\nimport torch","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\nmodel.train(\n    data='/kaggle/input/cv-project-files2/dataset.yaml',\n    epochs=75,                            # Number of training epochs\n    imgsz=640,                            # Image size\n    batch=32,                             # batch size\n    lr0=0.001,                             # initial learning rate\n    augment=True,                         # no data augmentation\n    dropout=0.2       ,                    # dropout to reduce overfitting\n    device=[0,1]\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-11-28T16:18:29.126358Z","iopub.status.idle":"2024-11-28T16:18:29.126672Z","shell.execute_reply.started":"2024-11-28T16:18:29.126504Z","shell.execute_reply":"2024-11-28T16:18:29.126518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from ultralytics import YOLO\n# resume training\n#model = YOLO('/kaggle/working/runs/detect/train/weights/last.pt')\n#model.train(resume=True)","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false,"scrolled":true,"execution":{"iopub.status.busy":"2024-11-28T16:18:29.128214Z","iopub.status.idle":"2024-11-28T16:18:29.128651Z","shell.execute_reply.started":"2024-11-28T16:18:29.128424Z","shell.execute_reply":"2024-11-28T16:18:29.128446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_folder = '/kaggle/input/large-license-plate-dataset/images/test/'\nmodel = YOLO('/kaggle/working/runs/detect/train/weights/best.pt')\n\nresults = model(image_folder,stream=False)\noutput_folder = '/kaggle/working/test'\nok = 0\nskip = 0\nfor image in results:\n    device = image.boxes.xywh.device\n    suitable_crops = torch.empty((0,6), device=device)\n    for i in range(len(image)):\n        x, y, w, h = image.boxes.xywh[i].tolist() \n        if w > 50 and h > 20:\n            conf = image.boxes.conf[i]\n            keep_crop = torch.tensor([[x-w/2,y-h/2,x+w/2,y+h/2,conf,0]],device=device)\n            suitable_crops = torch.cat((suitable_crops,keep_crop))\n            ok+=1\n        else:\n            skip+=1\n    image.update(suitable_crops)\n    image.save_crop(output_folder)\n\nprint(\"ok\",ok,\"skip\",skip)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:18:29.130032Z","iopub.status.idle":"2024-11-28T16:18:29.130478Z","shell.execute_reply.started":"2024-11-28T16:18:29.130244Z","shell.execute_reply":"2024-11-28T16:18:29.130268Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# to save the run : zip the folder from this command and then download the zip file\n#import shutil\n#shutil.make_archive(\"train_20\", 'zip', \"/kaggle/working/runs/detect/train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:18:29.131441Z","iopub.status.idle":"2024-11-28T16:18:29.131876Z","shell.execute_reply.started":"2024-11-28T16:18:29.131635Z","shell.execute_reply":"2024-11-28T16:18:29.131659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PaddleOCR","metadata":{}},{"cell_type":"code","source":"!pip install paddlepaddle-gpu\n!pip install paddleocr","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom paddleocr import PaddleOCR\nfrom PIL import Image\nimport pytesseract\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:48:28.209982Z","iopub.execute_input":"2024-12-04T19:48:28.210393Z","iopub.status.idle":"2024-12-04T19:48:32.326442Z","shell.execute_reply.started":"2024-12-04T19:48:28.210358Z","shell.execute_reply":"2024-12-04T19:48:32.325704Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def process_image(image):\n    scaled_image = cv2.resize(image, (400, 300))\n    gray_image = cv2.cvtColor(scaled_image, cv2.COLOR_BGR2GRAY)\n    gaussian_image = cv2.GaussianBlur(gray_image, (23, 23), 0)\n    \n    process_path = '/kaggle/working/PaddleOCR/preprocessed_license_plate.jpg'\n    cv2.imwrite(process_path, gaussian_image)\n    \n\ndef display_image(image, title=\"Image\", cmap='gray'):\n    plt.figure(figsize=(2, 2))  \n    plt.imshow(image, cmap=cmap) \n    plt.title(title) \n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:48:40.082008Z","iopub.execute_input":"2024-12-04T19:48:40.083039Z","iopub.status.idle":"2024-12-04T19:48:40.088066Z","shell.execute_reply.started":"2024-12-04T19:48:40.082987Z","shell.execute_reply":"2024-12-04T19:48:40.087170Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"csv_path = '/kaggle/input/license-plate-text-recognition-dataset/lpr.csv'  \nimages_folder = '/kaggle/input/license-plate-text-recognition-dataset/cropped_lps/cropped_lps'  \nprocess_path = '/kaggle/working/PaddleOCR/preprocessed_license_plate.jpg'\nocr = PaddleOCR(use_gpu=True,lang=\"en\",show_log=False, use_angle_cls=True)\n\ndata = pd.read_csv(csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:48:52.929746Z","iopub.execute_input":"2024-12-04T19:48:52.930084Z","iopub.status.idle":"2024-12-04T19:49:27.209724Z","shell.execute_reply.started":"2024-12-04T19:48:52.930053Z","shell.execute_reply":"2024-12-04T19:49:27.208734Z"}},"outputs":[{"name":"stdout","text":"download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3910/3910 [00:06<00:00, 603.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10000/10000 [00:05<00:00, 1963.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2138/2138 [00:15<00:00, 139.71it/s]","output_type":"stream"},{"name":"stdout","text":"[2024/12/04 19:49:24] ppocr WARNING: The first GPU is used for inference by default, GPU ID: 0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"[2024/12/04 19:49:25] ppocr WARNING: The first GPU is used for inference by default, GPU ID: 0\n[2024/12/04 19:49:26] ppocr WARNING: The first GPU is used for inference by default, GPU ID: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"correct_count = 0\nrecognized_count = 0\ntotal_count = 0\nalmost_count = 0\ni = 0\nfor index, row in tqdm(data.iterrows(), total=len(data), mininterval=2):    \n    if i < 10000:\n        i+=1\n        image_path = os.path.join(images_folder, row['images'])\n        ground_truth = row['labels']\n        image = cv2.imread(image_path)\n        \n        process_image(image)\n        result = ocr.ocr(process_path, rec=True)\n        if result and result[0]:\n            recognized_text = result[0][0][1][0]\n            recognized_text = re.sub(r'[^a-zA-Z0-9]', '', recognized_text)\n        else:\n            recognized_text = \"\"\n        \n        if recognized_text == ground_truth:\n            correct_count += 1\n            \n        total_count+=1\n    else:\n        break\n        \naccuracy = correct_count / total_count * 100\nprint(f\"Accuracy: {accuracy:.2f}% \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T19:49:27.211157Z","iopub.execute_input":"2024-12-04T19:49:27.211451Z","iopub.status.idle":"2024-12-04T19:55:37.314713Z","shell.execute_reply.started":"2024-12-04T19:49:27.211423Z","shell.execute_reply":"2024-12-04T19:55:37.313849Z"},"scrolled":true},"outputs":[{"name":"stderr","text":" 50%|█████     | 10000/20000 [06:10<06:10, 27.02it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 63.04% \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# EasyOCR","metadata":{}},{"cell_type":"code","source":"!pip install numpy\n!pip install easyocr\n!pip install opencv-python\n!pip install pandas\n!pip install numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:14:27.706617Z","iopub.execute_input":"2024-12-04T21:14:27.707256Z","iopub.status.idle":"2024-12-04T21:15:09.470690Z","shell.execute_reply.started":"2024-12-04T21:14:27.707214Z","shell.execute_reply":"2024-12-04T21:15:09.469554Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: easyocr in /opt/conda/lib/python3.10/site-packages (1.7.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from easyocr) (2.4.0)\nRequirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.19.0)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from easyocr) (4.10.0.84)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.14.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from easyocr) (10.3.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.23.2)\nRequirement already satisfied: python-bidi in /opt/conda/lib/python3.10/site-packages (from easyocr) (0.6.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from easyocr) (6.0.2)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.8.5.post1)\nRequirement already satisfied: pyclipper in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.3.0.post5)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from easyocr) (1.11.1.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (1.13.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->easyocr) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->easyocr) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->easyocr) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->easyocr) (1.3.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport easyocr\nimport cv2\nimport pandas as pd\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:15:34.796079Z","iopub.execute_input":"2024-12-04T21:15:34.796853Z","iopub.status.idle":"2024-12-04T21:15:34.801074Z","shell.execute_reply.started":"2024-12-04T21:15:34.796816Z","shell.execute_reply":"2024-12-04T21:15:34.800232Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"images_path = '/kaggle/input/license-plate-text-recognition-dataset/cropped_lps/cropped_lps'\ncsv_path = '/kaggle/input/license-plate-text-recognition-dataset/lpr.csv' \n\nreader = easyocr.Reader(['en'], gpu=True)\n\ndata = pd.read_csv(csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:15:37.400833Z","iopub.execute_input":"2024-12-04T21:15:37.401174Z","iopub.status.idle":"2024-12-04T21:15:40.423457Z","shell.execute_reply.started":"2024-12-04T21:15:37.401142Z","shell.execute_reply":"2024-12-04T21:15:40.422209Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def resize_image(image, target_height=200, target_width=600):\n    original_height, original_width = image.shape[:2]\n\n    scale_height = target_height / original_height\n    scale_width = target_width / original_width\n\n    scale_factor = min(scale_height, scale_width)\n\n    new_width = int(original_width * scale_factor)\n    new_height = int(original_height * scale_factor)\n\n    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n    return resized_image\n\n\n# def sharpen_image(image):\n#    kernel = np.array([[0, -1, 0],\n#                      [-1,  9, -1],\n#                      [0, -1, 0]])\n#    return cv2.filter2D(image, -1, kernel)\n\n#def denoise_image(image):\n#    return cv2.GaussianBlur(image, (5, 5), 0)\n\n\ndef process_image(image_path):\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Error: Image {image_path} not found or could not be loaded.\")\n        return None\n        \n    resized_img = resize_image(img)\n    \n    # gray_immg = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n    # gaussian_img = cv2.GaussianBlur(gray_image, (3, 3), 0)\n    # enhanced_img = enhance_contrast(resized_img)\n    # sharpened_img = sharpen_image(resized_img)\n    # denoised_img = denoise_image(img)\n    # gray_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n    \n    return resized_img\n\ndef calculate_cer(recognized_text, ground_truth):\n    incorrect_count = 0;\n\n    for i in range(min(len(recognized_text), len(ground_truth))):\n        if recognized_text[i] != ground_truth[i]:\n            incorrect_count += 1\n            \n    # counting any extra recognised characters as incorrect\n    incorrect_count += abs(len(recognized_text) - len(ground_truth))\n    return incorrect_count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:15:40.425098Z","iopub.execute_input":"2024-12-04T21:15:40.425453Z","iopub.status.idle":"2024-12-04T21:15:40.432647Z","shell.execute_reply.started":"2024-12-04T21:15:40.425424Z","shell.execute_reply":"2024-12-04T21:15:40.431833Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# initialising counters\ncorrect_count = 0\nrecognized_count = 0\ntotal_count = 0\nalmost_count = 0\ntotal_incorrect_chars = 0\ntotal_groundtruth_chars = 0\n\nfor index, row in data.iterrows():\n    if  total_count < 5000:\n        image_path = os.path.join(images_path, row['images'])\n        ground_truth = row['labels']\n\n        processed_img = process_image(image_path)\n        if processed_img is not None:\n            ocr_results_processed = reader.readtext(processed_img, detail=1, decoder='beamsearch')\n\n        recognized_text = \"\"\n        if ocr_results_processed:\n            for result in ocr_results_processed:\n                text, confidence = result[1], result[2]\n                if confidence > 0.5:  \n                    recognized_text = text.replace(\" \", \"\").replace(\"-\", \"\")\n                    recognized_count += 1\n\n        # normalising recognized text and ground truth for comparison\n        recognized_text = recognized_text.strip().lower()\n        ground_truth = ground_truth.strip().lower()\n\n        incorrect_chars = calculate_cer(recognized_text, ground_truth)\n        total_incorrect_chars += incorrect_chars\n        total_groundtruth_chars += len(ground_truth)\n        \n        if recognized_text == ground_truth:\n            correct_count += 1\n        elif recognized_text in ground_truth and recognized_text != \"\":\n            almost_count += 1\n\n        total_count += 1\n    else:\n        break\n\naccuracy = (correct_count / total_count) * 100\ncer = (total_incorrect_chars / total_groundtruth_chars) * 100 \nprint(\"Total: \", total_count)\nprint(f\"Accuracy: {correct_count} = {accuracy:.2f}%\")\nprint(f\"Character Error Rate (CER): {cer:.2f}%\")\nprint(f\"Recognized something: {recognized_count}\")\nprint(f\"Almost recognized: {almost_count} out of incorrects {total_count - correct_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T21:15:46.638914Z","iopub.execute_input":"2024-12-04T21:15:46.639752Z","iopub.status.idle":"2024-12-04T21:18:57.476535Z","shell.execute_reply.started":"2024-12-04T21:15:46.639717Z","shell.execute_reply":"2024-12-04T21:18:57.475637Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Total:  5000\nAccuracy: 618 = 12.36%\nCharacter Error Rate (CER): 78.63%\nRecognized something: 1780\nAlmost recognized: 145 out of incorrects 4382\n","output_type":"stream"}],"execution_count":16}]}